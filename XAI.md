# Weld & Bansal 2018 
## The Challenge of Crafting Intelligible Intelligence

Most computer-based produced behavior is alien (can fail in unexpected ways). Complex systems that exceeds human abilities to verify. We can neither trust nor control system behavior that we do not understand. 

Seek AI systems that:
1. It is clear what factors caused the system's action, allowing users to predict how changes to the situation would have led to alternative behaviors
2. Permits effective control of the AI by enabling interaction

Their Approach to solving this problem:
1) Ensuring that the underlying reasoning or learned model is inherently interpretable (e.g. by learning a linear model over a small number of well-understoof features)
2) If it is necessary to use an inscrutable model, such as complex neural networks or deep-lookahead search, then mapping this complex system to a simplier, explanatory model for understanding and control

Provides transparency and veracity: a user can see what the model is doing

Problem: Interpretable methods may not performa as well as more complex ones

Mapping approach can apply to best performing technique but its explanation inherently differs from the way the AI system actually operates.

Answer: Make the explanation system interactive so users can drill down until they are satisfied with their understanding

**The key challenge for designing intelligle AI is communicating a complex computation process to a human.** This requires interdisciplinary skills, including HCI as well as AI and machine learning expertise. 

## Why Intelligibility Matters
### AI May Have The Wrong Objective

### AI May Be Using Inadequate Features

### Distributional Drift

### Facilitating User Control

### User Acceptance

### Improving Human Insight

### Legal Imperatives

# Ribeiro, Singh, & Guestrin 2016 
## “Why Should I Trust You?” Explaining the Predictions of Any Classifier

# Lundberg & Lee (2017) 
## A Unified Approach to Interpreting Model Predictions

# Ribeiro, Singh, & Guestrin (2018) 
## Anchors: High-Precision Model-Agnostic Explanations

# Springer & Whittaker (2018)
## What Are You Hiding? Algorithmic Transparency and User Perceptions.
